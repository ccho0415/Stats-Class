---
title: "Lab 4"  
author: "Christine Chong cc4190"
date: "June 7th 2017"
output: 
  pdf_document: 
    latex_engine: lualatex
---

# Instructions 
Make sure that you upload an RMarkdown file to the canvas page (this should have a .Rmd extension) as well as the PDF output after you have knitted the file (this will have a .pdf extension).  Note that since you have already knitted this file, you should see both a **Lab4_UNI.pdf** and a **Lab4_UNI.Rmd** file in your UN4206 folder.  Click on the **Files** tab to the right to see this.  The files you upload to the Canvas page should be updated with commands you provide to answer each of the questions below.  You can edit this file directly to produce your final solutions.  The lab is due 11:59pm on Friday, June 9th.    

# Optimization


The goal of this lab is to write a simple optimization function in **R** which estimates the global minimum of a convex differentiable function $f(x)$.  Specifically, consider the function 
$$f(x)=\frac{-\log(x)}{1+x}, \ \ x>0,$$
where $\log(x)$ is the natural logarithm of $x$.  We seek to estimate the value of $x>0$ such that $f(x)$ achieves its global minimum.  For example, the global minimum of the function $g(x)=x^2-4x+3$ is at $x=2$. The minimum of $g(x)$ can easily be computed using the vertex formula for quadratic functions, i.e., $x=-b/(2a)=4/(2*1)=2$.  In most cases, the minimum does not have a closed form solution and must be computed numerically.  Hence we seek to estimate the global minimum of $f(x)$ numerically via gradient descent.    

# Tasks

1) Using **R**, define the function $$f(x)=\frac{-\log(x)}{1+x}, \ \ x>0.$$ Test the points $f(0)$ and $f(2)$.  
```{r}
convex_func <- function(x) {
  if(x>0){
    top <- (log(x)*-1)
    bottom <- 1+x
    total <- top/bottom
    return(total)    
  }else{
    return (FALSE)
  }

}
convex_func(0)
convex_func(2)
```
2) Plot the function $f(x)$ over the interval $(0,6]$. 
```{r}
allx <- c(1,2,3,4,5,6)
allx <- seq(.1,6,.01)

plot(allx,convex_func(allx), type="l")
```
3)  By inspection, were do you think global minimum is located at?  
  f(4)
  
4) Define a **R** function which computes the difference quotient of $f(x)$, i.e., for $h>0$, 
$$\frac{f(x+h)-f(x)}{h}.$$ This function should have two inputs; $h$ and $x$.  Name the difference quotient function **diff.quot**.  Note that for small $h$,  this function is the approximate derivative of $f(x)$.    

Use convex function to calculate derivative
```{r}
diff.quot <- function(x, h=.0001){
  if(x>0 && h>0){
    topdiff <- (convex_func(x+h) - convex_func(x))
    totaldiff <- topdiff/h
    return(totaldiff)
  }else{
    return(FALSE)
  }  
}
diff.quot(c(1,2,3))
```
5) Plot both the difference quotient function **diff.quot** and $f(x)$ over the interval $(0,6]$.  Fix $h=.0001$ to construct this plot.  Comment on any interesting features.
```{r}

plot(allx,diff.quot(allx), type ="l")
```
6) Write a **R** function named **basic.grad.descent** that runs the basic gradient descent algorithm on the function $f(x)$.
The function should have inputs:   
\begin{enumerate}
\item Initial value  {\bf x}
\item Maximum iterations  {\bf max.iter} with default 10000. 
\item Stopping criterion {\bf stop.deriv} with default 1e-10. 
\item Derivative step size {\bf h} with default .0001.  
\item Step size {\bf step.scale} with default .5. 
\end{enumerate} The function should have outputs:
\begin{enumerate}
\item The value $x$ that yields the minimum of $f(x)$. 
\item The minimum value of $f(x)$. 
\item The number of iterations the algorithm took to reach the minimum.
\item A logical indicator displaying whether or not the algorithm converged.  
\end{enumerate}
```{r}
basic.grad.descent <- function(x, h=.0001){
max.iter <- 10000
stop.deriv <- 1e-10
derivative.step <- h
step.scale <- .5
iter <- 0
deriv <- Inf
beta <- 0.15
sse <- function(b){ sum((convex_func(b) - diff.quot(b)*b^b))}
for(i in 1:max.iter){
  iter <- iter +1
}
  deriv <-(sse(beta+derivative.step) -sse(beta))/derivative.step
  beta <- beta -step.scale*deriv
  if(abs(deriv)<stop.deriv){break()}
fit <-list(beta=beta, iteration = iter, converged = (iter<max.iter)) 
return(x)
return(fit)

}

```
7)  Check the optimal value using the base **R** function **nlm()**.  
```{r}
nlm(basic.grad.descent, .001)
```

