xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
gd2.sparse<- sparse.grad.descent(huber.loss, c(0,0,0,0))
coef(gd2.sparse)
coef(lm(y~x))
b
sparse.grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < 0.05)) {
k <- k-1; break
}
sparse <- xmat[ ,k-1] - step.size * grad.cur
sparse[abs(sparse)< 0.05] <- 0
xmat[,k] <- sparse
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
gd2.sparse<- sparse.grad.descent(huber.loss, c(0,0,0,0))
coef(gd2.sparse)
coef(lm(y~x))
gd$x
gd2.sparse$x
gd2<- grad.descent(huber.loss, beta, max.iter = 200, step.size = 0.1)
gd2$k
gd2$x
obj <- apply(gd2$xmat[, 1:gd2$k], 2, huber.loss)plot((gd2$k-49):gd2$k, obj[(gd2$k- 49):gd2$k], xlab = "Iteration Number", ylab ="Objective Function Value", type = "l", main = "Objective Funct. Value During Gradient Descent")
gd2<- grad.descent(huber.loss, beta, max.iter = 200, step.size = 0.1)
gd2$k
gd2$x
obj <- apply(gd2$xmat[, 1:gd2$k], 2, huber.loss)
plot((gd2$k-49):gd2$k, obj[(gd2$k- 49):gd2$k], xlab = "Iteration Number", ylab ="Objective Function Value", type = "l", main = "Objective Funct. Value During Gradient Descent")
gdx<- gd$x
gdy<- seq(1,length(gdx), by=1 )
obj <- apply(gd$xmat[, 1:gd$k], 2, huber.loss)
plot(1:gd$k obj[1:gd$k], xlab = "Iteration Number", ylab ="Objective Function Value", type = "l", main = "Objective Funct. Value During Gradient Descent")
gdx<- gd$x
gdy<- seq(1,length(gdx), by=1 )
obj <- apply(gd$xmat[, 1:gd$k], 2, huber.loss)
plot(1:gd$k, obj[1:gd$k], xlab = "Iteration Number", ylab ="Objective Function Value", type = "l", main = "Objective Funct. Value During Gradient Descent")
gdx<- gd$x
gdy<- seq(1,length(gdx), by=1 )
obj <- apply(gd$xmat[, 1:gd$k], 2, huber.loss)
plot(1:gd$k, obj[1:gd$k], xlab = "K Number", ylab ="X Value", type = "l", main = "Value of X Over K Iterations")
gd2<- grad.descent(huber.loss, beta, max.iter = 200, step.size = 0.1)
gd2$k
gd2$x
obj <- apply(gd2$xmat[, 1:gd2$k], 2, huber.loss)
plot((gd2$k-49):gd2$k, obj[(gd2$k- 49):gd2$k], xlab = "K Number", ylab ="X Value", type = "l", main = "Value of X Over K Iterations")
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, x0=beta)
gd$k
gd$x
obj <- apply(gd2$xmat[, 1:gd2$k], 2, huber.loss)
plot((gd2$k-49):gd2$k, obj[(gd2$k- 49):gd2$k], xlab = "K Number", ylab ="X Value", type = "l", main = "Value of X Over K Iterations")
coef(lm(y~x))
gd$x
gd2.sparse$x
set.seed(10)
beta = rep(0,p)
gd3<-grad.descent(huber.loss, beta)
gd3.sparse<-sparse.grad.descent(huber.loss, beta)
set.seed(10)
beta = rep(0,p)
gd3<-grad.descent(huber.loss, beta)
gd3.sparse<-sparse.grad.descent(huber.loss, beta)
gd3$x
gd3$k
gd3.sparse$x
gd3.sparse$k
for(i in 1:10){
set.seed(sample(1:100, 1))
beta = rep(0,p)
gd[i]<- grad.descent(huber.loss, beta)
gd[i].sparse <- sparse.grad.descent(huber.loss, beta)
for(i in 1:10){
set.seed(sample(1:100, 1))
beta = rep(0,p)
gd[i]<- grad.descent(huber.loss, beta);
gd[i].sparse <- sparse.grad.descent(huber.loss, beta);
for(i in 1:10){
set.seed(sample(1:100, 1))
beta = rep(0,p)
allgd[i]<- grad.descent(huber.loss, beta);
allgdsparse[i] <- sparse.grad.descent(huber.loss, beta);
print(gd[i]$x)
print(gd[i]$k)
print(gd[i].sparse$x)
for(i in 1:10){
set.seed(sample(1:100, 1))
beta = rep(0,p)
allgd[i]<- grad.descent(huber.loss, beta);
allgdsparse[i] <- sparse.grad.descent(huber.loss, beta);
print(allgd[i]$x)
print(allgd[i]$k)
print(allgdsparse[i])
print(allgdsparse[i])
}
for(i in 1:10){
allgd<- c()
allgdsparse <- c()
set.seed(sample(1:100, 1))
beta = rep(0,p)
allgd[i]<- grad.descent(huber.loss, beta);
allgdsparse[i] <- sparse.grad.descent(huber.loss, beta);
print(allgd[i]$x)
print(allgd[i]$k)
print(allgdsparse[i])
print(allgdsparse[i])
}
knitr::opts_chunk$set(echo = TRUE)
library(numDeriv)
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rt(n, df=2)
cors <- apply (x, 2 , cor, y)
cors
a<- seq(-5,5, length = 100)
ha <-dnorm(a)
degf <- c(1,3,8)
colors <- c("red", "green", "blue", "black")
labels <- c("1st : 1", "2nd : 3", "3rd : 8", "normal")
plot(a,ha, type ="l", lty=2, xlab="x value", ylab="Density", main ="Comparision of Normal and T")
for(i in 1:length(degf)){
lines(a, dt(a, degf[i]), lwd = 2, col = colors[i])
}
legend("topright", title = "Distributions", labels, lwd=2, lty = c(1,1,1,2), col = colors )
psi <- function(r, c = 1) {
return(ifelse(r^2 > c^2, 2*c*abs(r) - c^2, r^2))
}
huber.loss<- function(beta){
n <- 100
p <- length(beta)
x <- matrix(rnorm(n*p), n, p)
b <- beta
y <- x %*% b + rt(n, df=2)
model <- lm(y~x)
sumThis<- sum(psi(resid(model)))
return(sumThis)
}
vec <- rep(0,5)
huber.loss(vec)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, x0=beta)
gd$k
gd$x
huber.loss<- function(beta){
b <- beta
model <- (y-x)
sumThis<- sum(psi(model))
return(sumThis)
}
vec <- rep(0,5)
huber.loss(vec)
huber.loss<- function(beta){
b <- beta
model <- (y-x %*% b)
sumThis<- sum(psi(model))
return(sumThis)
}
vec <- rep(0,5)
huber.loss(vec)
huber.loss<- function(beta){
b <- beta
model <- (y - x %*% b)
sumThis<- sum(psi(model))
return(sumThis)
}
vec <- rep(0,5)
huber.loss(vec)
huber.loss<- function(beta){
b <- beta
sumThis<- sum(psi(y - x %*% b))
return(sumThis)
}
vec <- rep(0,5)
huber.loss(vec)
huber.loss<- function(beta){
sumThis<- sum(psi(y - x %*% beta))
return(sumThis)
}
vec <- rep(0,5)
huber.loss(vec)
knitr::opts_chunk$set(echo = TRUE)
library(numDeriv)
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rt(n, df=2)
cors <- apply (x, 2 , cor, y)
cors
a<- seq(-5,5, length = 100)
ha <-dnorm(a)
degf <- c(1,3,8)
colors <- c("red", "green", "blue", "black")
labels <- c("1st : 1", "2nd : 3", "3rd : 8", "normal")
plot(a,ha, type ="l", lty=2, xlab="x value", ylab="Density", main ="Comparision of Normal and T")
for(i in 1:length(degf)){
lines(a, dt(a, degf[i]), lwd = 2, col = colors[i])
}
legend("topright", title = "Distributions", labels, lwd=2, lty = c(1,1,1,2), col = colors )
psi <- function(r, c = 1) {
return(ifelse(r^2 > c^2, 2*c*abs(r) - c^2, r^2))
}
huber.loss<- function(beta){
sumThis<- sum(psi(y - x %*% b))
return(sumThis)
}
vec <- rep(0,5)
huber.loss(vec)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, x0=beta)
huber.loss<- function(beta){
return(sum(psi(y - x %*% b)))
}
vec <- rep(0,5)
huber.loss(vec)
huber.loss<- function(beta){
return((psi(y - x %*% b)))
}
vec <- rep(0,5)
huber.loss(vec)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, x0=beta)
huber.loss<- function(beta){
return(sum(psi(y - x %*% b)))
}
vec <- rep(0,5)
huber.loss(vec)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, x0=beta)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, beta)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss,beta)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = unlist(rep(0,p))
gd<- grad.descent(huber.loss,beta)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss(beta),beta)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss,beta)
logistic.Neg.LL <- function(b,data=data) {
b0 <- b[1]
b1 <- b[2]
x <- data$x
y <- data$y
#The corresponding probalities that the x matches y
p.i <- exp(b0+b1*x)/(1+exp(b0+b1*x))
#Use Negative so we can minimizing instead of maximizing
return(-sum(dbinom(y,size=1,prob=p.i,log=TRUE)))
}
logistic.Neg.LL(b=c(0,0),data=data)
logistic.Neg.LL(b=c(-3,.6),data=data)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
grad.cur <- grad(f, xmat[ ,k-1], ...)
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k]
return(list(x = xmat[,k], xmat = xmat, k = k))
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, beta)
logistic.Neg.LL <- function(b,data=data) {
b0 <- b[1]
b1 <- b[2]
x <- data$x
y <- data$y
#The corresponding probalities that the x matches y
p.i <- exp(b0+b1*x)/(1+exp(b0+b1*x))
#Use Negative so we can minimizing instead of maximizing
return(-sum(dbinom(y,size=1,prob=p.i,log=TRUE)))
}
logistic.Neg.LL(b=c(0,0),data=data)
logistic.Neg.LL(b=c(-3,.6),data=data)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.001, stopping.deriv = 0.1, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k],
xmat = xmat,
k = k,
minimum=f(xmat[,k],...)
)
)
}
beta = rep(0,p)
gd<- grad.descent(huber.loss, beta)
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
coef(model)
model <- glm(y~x,data=data,family=binomial(link = "logit"))
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k],
xmat = xmat,
k = k,
minimum=f(xmat[,k],...)
)
)
}
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
coef(model)
data <- read.table("Example71.txt")
#########################
# Plot of raw data
#########################
plot(data$x,data$y,xlab="Tumor size (cm), X", ylab="Lymph node metastasis, Y",main="Raw Data",ylim=c(-.2,1.2),xlim=c(0,12))
abline(h=0,lty=3)
abline(h=1,lty=3)
abline(v=min(data$x),lty=2,col=1)
abline(v=max(data$x),lty=2,col=1)
#########################
# Estimate logistic model usinf glm()
#########################
#If you run the glm (general linear models) function then you have to link it
model <- glm(y~x,data=data,family=binomial(link = "logit"))
data <- read.table("Example71.txt")
data <- read.table("Example71.txt")
getwd()
getwd()
