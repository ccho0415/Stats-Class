knitr::opts_chunk$set(echo = TRUE)
library(numDeriv)
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rt(n, df=2)
a<- seq(-5,5, length = 1000)
ha <-pnorm(a)
degf <- c(1,3,8)
colors <- c("red", "green", "blue", "black")
labels <- c("1st : 1", "2nd : 3", "3rd : 8", "normal")
plot(a,ha, type ="l", lty=2, xlab="x value", ylab="Density", main ="Comparision of Normal and T")
for(i in 1:length(degf)){
lines(a, dt(a, degf[i]), lwd = 2, col = colors[i])
}
legend("topright", title = "Distributions", labels, lwd=2, lty = c(1,1,1,2), col = colors )
a<- seq(-5,5, length = 1000)
ha <-dnorm(a)
degf <- c(1,3,8)
colors <- c("red", "green", "blue", "black")
labels <- c("1st : 1", "2nd : 3", "3rd : 8", "normal")
plot(a,ha, type ="l", lty=2, xlab="x value", ylab="Density", main ="Comparision of Normal and T")
for(i in 1:length(degf)){
lines(a, dt(a, degf[i]), lwd = 2, col = colors[i])
}
legend("topright", title = "Distributions", labels, lwd=2, lty = c(1,1,1,2), col = colors )
a<- seq(-5,5, length = 1000)
ha <-pnorm(a)
degf <- c(1,3,8)
colors <- c("red", "green", "blue", "black")
labels <- c("1st : 1", "2nd : 3", "3rd : 8", "normal")
plot(a,ha, type ="l", lty=2, xlab="x value", ylab="Density", main ="Comparision of Normal and T")
for(i in 1:length(degf)){
lines(a, dt(a, degf[i]), lwd = 2, col = colors[i])
}
legend("topright", title = "Distributions", labels, lwd=2, lty = c(1,1,1,2), col = colors )
a<- seq(-5,5, length = 1000)
ha <-dnorm(a)
degf <- c(1,3,8)
colors <- c("red", "green", "blue", "black")
labels <- c("1st : 1", "2nd : 3", "3rd : 8", "normal")
plot(a,ha, type ="l", lty=2, xlab="x value", ylab="Density", main ="Comparision of Normal and T")
for(i in 1:length(degf)){
lines(a, dt(a, degf[i]), lwd = 2, col = colors[i])
}
legend("topright", title = "Distributions", labels, lwd=2, lty = c(1,1,1,2), col = colors )
setwd("~/Desktop/Data")
data <- read.table("Example71.txt")
#########################
# Plot of raw data
#########################
plot(data$x,data$y,xlab="Tumor size (cm), X", ylab="Lymph node metastasis, Y",main="Raw Data",ylim=c(-.2,1.2),xlim=c(0,12))
abline(h=0,lty=3)
abline(h=1,lty=3)
abline(v=min(data$x),lty=2,col=1)
abline(v=max(data$x),lty=2,col=1)
model <- glm(y~x,data=data,family=binomial(link = "logit"))
linear.pred <- predict(model,newdata=data.frame(x=7))
probs <- exp(linear.pred)/(1+exp(linear.pred))
probs
plot(data$x,data$y,xlab="Tumor size (cm), X", ylab="Lymph node metastasis, Y",main="Raw Data",ylim=c(-.2,1.2),xlim=c(0,12))
abline(h=0,lty=3)
abline(h=1,lty=3)
abline(v=min(data$x),lty=2,col=1)
abline(v=max(data$x),lty=2,col=1)
x.pred <- seq(-1,13,by=.1)
linear.pred <- predict(model,newdata=data.frame(x=x.pred))
probs <- exp(linear.pred)/(1+exp(linear.pred))
lines(x.pred,probs,col="purple")
linear.pred <- predict(model,newdata=data.frame(x=7))
probs <- exp(linear.pred)/(1+exp(linear.pred))
probs
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
logistic.Neg.LL <- function(b,data=data) {
b0 <- b[1]
b1 <- b[2]
x <- data$x
y <- data$y
p.i <- exp(b0+b1*x)/(1+exp(b0+b1*x))
return(-sum(dbinom(y,size=1,prob=p.i,log=TRUE)))
}
logistic.Neg.LL(b=c(0,0),data=data)
logistic.Neg.LL(b=c(-3,.6),data=data)
#########################
# grad.descent from class
#########################
library(numDeriv)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k],
xmat = xmat,
k = k,
minimum=f(xmat[,k],...)
)
)
}
#########################
# Optimization using grad.descent()
#########################
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
coef(model)
data <- read.table("Example71.txt")
model <- glm(y~x,data=data,family=binomial(link = "logit"))
library(numDeriv)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k],
xmat = xmat,
k = k,
minimum=f(xmat[,k],...)
)
)
}
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
coef(model)
logistic.Neg.LL <- function(b,data=data) {
b0 <- b[1]
b1 <- b[2]
x <- data$x
y <- data$y
#The corresponding probalities that the x matches y
p.i <- exp(b0+b1*x)/(1+exp(b0+b1*x))
#Use Negative so we can minimizing instead of maximizing
return(-sum(dbinom(y,size=1,prob=p.i,log=TRUE)))
}
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
coef(model)
return(list(x = xmat[,k],
xmat = xmat,
k = k,
minimum=f(xmat[,k],...)
)
)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
Newton.Method <- function(f, x0, max.iter = 200, stopping.deriv = 0.01, ...){
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
#Calculate the hessian
hess.cur <- hessian(f, xmat[ ,k-1], ... )
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
#Use Inverse of hess.cur and matrix multiply with grad curve
xmat[ ,k] <- xmat[ ,k-1] - solve(hess.cur)%*%grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k],
xmat = xmat,
k = k,
minimum=f(xmat[,k],...)
)
)
}
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
x0 <- c(0,0)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
x0 <- c(0,0)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
x0 <- c(0,0)
gd <- grad.descent(logistic.Neg.LL,x0,max.iter = 2000, step.size = 0.01, stopping.deriv = 0.001,data=data)
gd$minimum
gd$k
gd$x
coef(model)
x0 <- c(0,0)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
x0 <- c(0,0)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
nml.opt <- nlm(logistic.Neg.LL,c(0,0),data=data)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
x0 <- c(0,0)
nm <- grad.descent(Newton.Method,x0,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
x1 <- c(0,0)
nm <- grad.descent(Newton.Method,x1,max.iter = 2000, stopping.deriv = 0.001,data=data)
nm$minimum
nm$k
nm$x
coef(model)
